{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f3a0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sandy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import re \n",
    "import csv\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "262051b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取training.txt文件\n",
    "def load_training_data(training_file, data_directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    training_indices = []\n",
    "    with open(training_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            file_indices = parts[1:]\n",
    "            training_indices.extend(int(index) for index in file_indices)\n",
    "            for index in file_indices:\n",
    "                file_path = os.path.join(data_directory, f\"{index}.txt\")\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data.append(f.read())\n",
    "                labels.append(class_id)\n",
    "    return data, labels, training_indices\n",
    "\n",
    "# 讀取測試數據\n",
    "def load_test_data(data_directory, training_indices):\n",
    "    test_data = []\n",
    "    test_indices = [i for i in range(1, 1096) if i not in training_indices]\n",
    "    for index in test_indices:\n",
    "        file_path = os.path.join(data_directory, f\"{index}.txt\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            test_data.append(f.read())\n",
    "    return test_data, test_indices\n",
    "\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "# From hw2\n",
    "def preprocess_text(texts): \n",
    "    processed_texts = [] \n",
    "    punctuation = ',.!?;:\"`()_' \n",
    "    punctuation += \"'\" \n",
    "    stemmer = PorterStemmer() \n",
    "    for text in texts: \n",
    "        for char in punctuation: \n",
    "            text = text.replace(char, '') \n",
    "        tokens = text.lower().split()\n",
    "        stemmed_words = [stemmer.stem(word) for word in tokens] \n",
    "        processed_texts.append([word for word in stemmed_words if word not in stopwords]) \n",
    "    return processed_texts\n",
    "\n",
    "training_file = 'training.txt'\n",
    "data_directory = 'data'\n",
    "train_data, train_labels, training_indices = load_training_data(training_file, data_directory)\n",
    "test_data, test_indices = load_test_data(data_directory, training_indices)\n",
    "\n",
    "train_data = preprocess_text(train_data)\n",
    "test_data = preprocess_text(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac6ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_frequencies(train_data, train_labels):\n",
    "    word_freqs = defaultdict(lambda: defaultdict(int))  # 保存每個類別中每個單詞的頻率\n",
    "    class_counts = 15\n",
    "    vocab = set()  # 所有出現在訓練數據中的單詞\n",
    "    doc_counts = defaultdict(lambda: defaultdict(int))  # 保存每個類別中每個單詞有出現的文檔數\n",
    "\n",
    "    for words, label in zip(train_data, train_labels):\n",
    "        seen_words = set()  # 跟踪本文檔中已經見過的單詞\n",
    "        for word in words:\n",
    "            word_freqs[label][word] += 1\n",
    "            vocab.add(word)\n",
    "            if word not in seen_words:\n",
    "                doc_counts[label][word] += 1  \n",
    "                seen_words.add(word)\n",
    "\n",
    "    return word_freqs, class_counts, vocab, doc_counts\n",
    "\n",
    "word_freqs, class_counts, vocab, doc_counts = calculate_word_frequencies(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46a5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_test(word_freqs, class_counts, doc_counts, vocab):\n",
    "    chi2_scores = defaultdict(float)\n",
    "    N = 13 * 15\n",
    "    \n",
    "    for word in vocab:\n",
    "        for class_id in range(class_counts):\n",
    "            TP = doc_counts[class_id][word]  # 該類別中包含該單詞的文檔數量\n",
    "            FP = sum(doc_counts[c][word] for c in range(class_counts) if c != class_id)  # 其他類別中包含該單詞的文檔數量\n",
    "            TN = 15 - TP  # 該類別中不包含該單詞的文檔數量\n",
    "            FN = N - (TP + TN + FP)  # 其他類別中不包含該單詞的文檔數量\n",
    "#             print(TP, \", \", TN, \", \", FP, \", \", FN)\n",
    "            \n",
    "            if (TP + FP) * (TN + FN) * (TP + TN) * (FP + FN) > 0:  # 防止分母為0\n",
    "                chi2 = N * ((TP * FN - TN * FP) ** 2) / ((TP + FP) * (TN + FN) * (TP + TN) * (FP + FN))\n",
    "                chi2_scores[word] += chi2\n",
    "    \n",
    "    return chi2_scores\n",
    "\n",
    "\n",
    "chi2_scores = chi2_test(word_freqs, class_counts,doc_counts , vocab)\n",
    "top_500_features = sorted(chi2_scores, key=chi2_scores.get, reverse=True)[:500]\n",
    "# print(f\"選擇的前500個特徵: {top_500_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ef113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # frequency based\n",
    "# def load_dictionary(dictionary_file):\n",
    "#     term_frequencies = defaultdict(int)\n",
    "\n",
    "#     with open(dictionary_file, 'r', encoding='utf-8') as file:\n",
    "#         for line in file:\n",
    "#             parts = line.strip().split()\n",
    "#             term = parts[1]\n",
    "#             freq = int(parts[2])\n",
    "#             term_frequencies[term] += freq\n",
    "    \n",
    "#     return term_frequencies\n",
    "\n",
    "# dictionary_file = 'dictionary.txt'# From hw2\n",
    "# term_frequencies = load_dictionary(dictionary_file)\n",
    "\n",
    "# def select_top_features(term_frequencies, top_n=500):\n",
    "#     sorted_terms = sorted(term_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "#     top_features = [term for term, freq in sorted_terms[:top_n]]\n",
    "#     return top_features\n",
    "\n",
    "# top_500_features = select_top_features(term_frequencies, top_n=500)\n",
    "# # print(f\"選擇的前500個特徵: {top_500_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97229d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算條件概率 P(X=t|C)\n",
    "def calculate_conditional_probs(word_freqs, class_counts, vocab, alpha=1):\n",
    "    conditional_probs = defaultdict(lambda: defaultdict(float))\n",
    "    for label in range(class_counts):\n",
    "        total_words = sum(word_freqs[label].values())\n",
    "        for word in vocab: #smoothing\n",
    "            conditional_probs[label][word] = (word_freqs[label][word] + alpha) / (total_words + alpha * len(vocab))\n",
    "    return conditional_probs\n",
    "\n",
    "conditional_probs = calculate_conditional_probs(word_freqs, class_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1651679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(document, conditional_probs, class_counts, vocab):\n",
    "    max_log_prob = float('-inf')\n",
    "    best_class = None\n",
    "    for label in range(class_counts):\n",
    "        log_prob = log(1/13) #P(C)\n",
    "        for word in document:\n",
    "            if word in vocab:\n",
    "                log_prob += np.log(conditional_probs[label][word]+1e-10)\n",
    "        if log_prob > max_log_prob:\n",
    "            max_log_prob = log_prob\n",
    "            best_class = label\n",
    "    return best_class\n",
    "\n",
    "predictions = [classify(doc, conditional_probs, class_counts, top_500_features) for doc in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50032f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 results.csv\n"
     ]
    }
   ],
   "source": [
    "def output_results(predictions, test_indices, output_file='results.csv'):\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Id', 'Value'])\n",
    "        for id_, pred in zip(test_indices, predictions):\n",
    "            writer.writerow([id_, pred])\n",
    "\n",
    "output_results(predictions, test_indices)\n",
    "print(\"已保存 results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec0944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee1383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
